{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Language Translation using BERT :**\n",
        "\n",
        "For the language translation app, the model we use is mBART(Multilingual BART)\n",
        "\n",
        "**mBART** is a sequence-to-sequence model that was specifically designed to work with multiple languages. It has been pre-trained on 50 languages and can translate between them. This is exactly what you need when building a translation system that supports multiple languages, as you can translate any language to any other language."
      ],
      "metadata": {
        "id": "1ax7HJBnw6cB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries"
      ],
      "metadata": {
        "id": "beiI5LmCw6jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast"
      ],
      "metadata": {
        "id": "dv0-hLy8xq_r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MBartForConditionalGeneration** : This is the model class for mBART, which is used for tasks that involve generating a sequence of text based on an input sequence. It is particularly used for sequence-to-sequence tasks, such as machine translation.\n",
        "\n",
        "- **MBart50TokenizerFast** :This is the tokenizer class that converts the input text into tokenized form that can be fed into the MBartForConditionalGeneration model. Tokenization is the process of splitting text into smaller units (tokens) that the model can understand, and the MBart50TokenizerFast is a fast, optimized version of the tokenizer for mBART."
      ],
      "metadata": {
        "id": "WW_yxUaMx0Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model and tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "8udj9RSpxvhE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "efSbmSSQyV9I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, source_lang, target_lang):\n",
        "    \"\"\"\n",
        "    Translate the given text from source language to target language using mBART.\n",
        "    \"\"\"\n",
        "    tokenizer.src_lang = source_lang  # Set source language (e.g., 'en_XX' for English)\n",
        "    model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Translate the input text\n",
        "    translated = model.generate(**model_inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang])\n",
        "\n",
        "    # Decode the translated text\n",
        "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "VLIN0Ufmybzk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "translated_text = translate(\"Hello, how are you?\", \"en_XX\", \"mr_IN\")\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0PcY4JJytns",
        "outputId": "2409559d-f727-4256-8cb9-4338e4c7c90e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "नमस ् ते, आपण कसे आहात?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for Hindi to English\n",
        "translated_text_hindi = translate(\"नमस्कार, आप कैसे हैं?\", \"hi_IN\", \"en_XX\")\n",
        "print(translated_text_hindi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnJmSswQzTEL",
        "outputId": "caf725de-a0b2-4962-beb1-82ece494f3fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model an tokenizer using pickle"
      ],
      "metadata": {
        "id": "BCpA2jlp2D0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenizer.pkl', \"wb\") as tokenizer_file :\n",
        "  pickle.dump(tokenizer, tokenizer_file)"
      ],
      "metadata": {
        "id": "-z1Pobln0WXp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the standard way to save PyTorch models.\n",
        "torch.save(model.state_dict(), \"model_weights.pth\")"
      ],
      "metadata": {
        "id": "KEf0xFnb0t9N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzZDIkt92dJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}